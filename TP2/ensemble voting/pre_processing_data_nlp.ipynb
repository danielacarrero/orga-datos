{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import re\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 54 columns):\n",
      "keyword                              7552 non-null object\n",
      "location                             5080 non-null object\n",
      "text                                 7613 non-null object\n",
      "keyword_grouped                      7552 non-null object\n",
      "text_contain_keyword                 7613 non-null bool\n",
      "total_words                          7613 non-null int64\n",
      "total_upper_chars                    7613 non-null int64\n",
      "total_numbers_chars                  7613 non-null int64\n",
      "total_special_chars                  7613 non-null int64\n",
      "contain_question                     7613 non-null bool\n",
      "contain_link                         7613 non-null bool\n",
      "contain_hashtag                      7613 non-null bool\n",
      "contain_upper_words                  7613 non-null bool\n",
      "total_3_words                        7613 non-null int64\n",
      "total_4_words                        7613 non-null int64\n",
      "total_5_words                        7613 non-null int64\n",
      "total_6_words                        7613 non-null int64\n",
      "total_7_words                        7613 non-null int64\n",
      "total_8_words                        7613 non-null int64\n",
      "total_3_ormore_words                 7613 non-null int64\n",
      "total_4_ormore_words                 7613 non-null int64\n",
      "total_5_ormore_words                 7613 non-null int64\n",
      "total_6_ormore_words                 7613 non-null int64\n",
      "total_8_ormore_words                 7613 non-null int64\n",
      "total_3_orless_words                 7613 non-null int64\n",
      "total_4_orless_words                 7613 non-null int64\n",
      "total_5_orless_words                 7613 non-null int64\n",
      "total_6_orless_words                 7613 non-null int64\n",
      "total_8_orless_words                 7613 non-null int64\n",
      "subjectivity_text                    7613 non-null float64\n",
      "polarity_text                        7613 non-null float64\n",
      "stopword_count                       7613 non-null int64\n",
      "unique_word_count                    7613 non-null int64\n",
      "text_contain_word_location           7613 non-null bool\n",
      "len_location_cero_default            7613 non-null int64\n",
      "len_location_mean_default            7613 non-null int64\n",
      "total_words_location_cero_default    7613 non-null int64\n",
      "total_words_location_mean_default    7613 non-null int64\n",
      "text_contain_keyword_similarity      7613 non-null bool\n",
      "text_similarity_keyword              7613 non-null int64\n",
      "text_best_similarity_keyword         7613 non-null int64\n",
      "text_similarity_location             7613 non-null int64\n",
      "text_best_similarity_location        7613 non-null int64\n",
      "contain_words_100_true               7613 non-null bool\n",
      "contain_words_90_true                7613 non-null bool\n",
      "contain_words_90_false               7613 non-null bool\n",
      "contain_words_85_true                7613 non-null bool\n",
      "contain_words_85_false               7613 non-null bool\n",
      "contain_words_80_true                7613 non-null bool\n",
      "contain_words_80_false               7613 non-null bool\n",
      "contain_words_75_true                7613 non-null bool\n",
      "contain_words_70_true                7613 non-null bool\n",
      "contain_words_70_false               7613 non-null bool\n",
      "target                               7613 non-null int64\n",
      "dtypes: bool(17), float64(2), int64(31), object(4)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/train_pre_processing.csv')\n",
    "test = pd.read_csv('data/test_pre_processing.csv')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    text = re.compile(r'https?://\\S+|www\\.\\S+').sub(r' ', text)\n",
    "    text = re.compile(r'http\\S+').sub(r'', text)\n",
    "    text = re.compile(r'www\\S+').sub(r'', text)\n",
    "    text = re.compile(r'pic.twitter.com\\S+').sub(r' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [w for w in words if not w in STOPWORDS]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "def spellcheck(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_non_ascii(text):\n",
    "    return re.compile(r'[^A-Za-z0-9\\.\\'!\\?,\\$]').sub(r' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spaces(text):\n",
    "    # Reemplazar multiples espacios en uno\n",
    "    return re.sub('\\s{2,}', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = remove_url(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = clean_non_ascii(text)\n",
    "    text = clean_spaces(text)\n",
    "#    text = spellcheck(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text_clean'] = train.text.transform(lambda x: clean_text(str(x)))\n",
    "test['text_clean'] = test.text.transform(lambda x: clean_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7141    shoes Asics GTII Super Red 20 11 Ronnie Fieg Kith Red White 3M x gel grey volcano 2               \n",
       "1987    fitness Knee Damage Solution                                                                      \n",
       "7256    I stand alone dont piss moan choices made If I must reap whirlwind Ill demeanor calm staid        \n",
       "5942    I screamed fuck hond                                                                              \n",
       "4943    ltmeltdown proportions commences I manage calm long enough turn waters hot wait steam cloud vision\n",
       "Name: text_clean, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text_clean.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['mean_word_len'] = train.text.transform(lambda x: np.mean([len(word) for word in str(x).split()]))\n",
    "test['mean_word_len'] = test.text.transform(lambda x: np.mean([len(word) for word in str(x).split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text, n_gram=1):\n",
    "    token = []\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        filter_words = STOPWORDS.union(set(string.punctuation))\n",
    "        if w not in filter_words:\n",
    "            token.append(w)\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_bigrams = defaultdict(int)\n",
    "nondisaster_bigrams = defaultdict(int)\n",
    "\n",
    "for tweet in train.loc[train['target'] == 1]['text_clean']:\n",
    "    for word in get_ngrams(tweet, 2):\n",
    "        disaster_bigrams[word] += 1\n",
    "        \n",
    "for tweet in train.loc[train['target'] == 0]['text_clean']:\n",
    "    for word in get_ngrams(tweet, 2):\n",
    "        nondisaster_bigrams[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos mantenemos con los que tengan una frecuencia mayor o igual a 10 en desastre\n",
    "delete_keys = [k for k, v in disaster_bigrams.items() if v < 10]\n",
    "for key in delete_keys:\n",
    "    del disaster_bigrams[key]\n",
    "\n",
    "delete_keys = [k for k, v in nondisaster_bigrams.items() if v < 10]\n",
    "for key in delete_keys:\n",
    "    del nondisaster_bigrams[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature booleano para saber si el texto tiene un bigrama de desastre o no\n",
    "\n",
    "train['text_disaster_bigram'] = train.text_clean.transform(lambda x: any(disngram in disaster_bigrams.keys() for disngram in get_ngrams(x, 2)))\n",
    "train['text_nondisaster_bigram'] = train.text_clean.transform(lambda x: any(disngram in nondisaster_bigrams.keys() for disngram in get_ngrams(x, 2)))\n",
    "test['text_disaster_bigram'] = test.text_clean.transform(lambda x: any(disngram in disaster_bigrams.keys() for disngram in get_ngrams(x, 2)))\n",
    "test['text_nondisaster_bigram'] = test.text_clean.transform(lambda x: any(disngram in nondisaster_bigrams.keys() for disngram in get_ngrams(x, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6626\n",
       "True     987 \n",
       "Name: text_disaster_bigram, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text_disaster_bigram'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6693\n",
       "True     920 \n",
       "Name: text_nondisaster_bigram, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text_nondisaster_bigram'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_trigrams = defaultdict(int)\n",
    "nondisaster_trigrams = defaultdict(int)\n",
    "\n",
    "for tweet in train.loc[train['target'] == 1]['text_clean']:\n",
    "    for word in get_ngrams(tweet, 3):\n",
    "        disaster_trigrams[word] += 1\n",
    "        \n",
    "for tweet in train.loc[train['target'] == 0]['text_clean']:\n",
    "    for word in get_ngrams(tweet, 2):\n",
    "        nondisaster_trigrams[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos mantenemos con los que tengan una frecuencia mayor o igual a 3\n",
    "delete_keys = [k for k, v in disaster_trigrams.items() if v < 3]\n",
    "for key in delete_keys:\n",
    "    del disaster_trigrams[key]\n",
    "\n",
    "delete_keys = [k for k, v in nondisaster_trigrams.items() if v < 3]\n",
    "for key in delete_keys:\n",
    "    del nondisaster_trigrams[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature booleano para saber si el texto tiene un bigrama de desastre o no\n",
    "\n",
    "train['text_disaster_trigram'] = train.text_clean.transform(lambda x: any(disngram in disaster_trigrams.keys() for disngram in get_ngrams(x, 3)))\n",
    "train['text_nondisaster_trigram'] = train.text_clean.transform(lambda x: any(disngram in nondisaster_trigrams.keys() for disngram in get_ngrams(x, 2)))\n",
    "test['text_disaster_trigram'] = test.text_clean.transform(lambda x: any(disngram in disaster_trigrams.keys() for disngram in get_ngrams(x, 3)))\n",
    "test['text_nondisaster_trigram'] = test.text_clean.transform(lambda x: any(disngram in nondisaster_trigrams.keys() for disngram in get_ngrams(x, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6408\n",
       "True     1205\n",
       "Name: text_disaster_trigram, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text_disaster_trigram'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    5306\n",
       "True     2307\n",
       "Name: text_nondisaster_trigram, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text_nondisaster_trigram'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    norm='l2',\n",
    "    min_df=0,\n",
    "    smooth_idf=False,\n",
    "    max_features=5000)\n",
    "X = vectorizer.fit_transform(train['text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Columns: 5057 entries, keyword to zss\n",
      "dtypes: bool(21), float64(5000), int64(31), object(5)\n",
      "memory usage: 292.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(X.todense(), columns = vectorizer.get_feature_names())\n",
    "df_tfidf.drop(labels = ['location','text', 'target'], axis=1, inplace=True)\n",
    "train = train.join(df_tfidf)\n",
    "print(train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Columns: 5056 entries, id to zss\n",
      "dtypes: bool(21), float64(4999), int64(31), object(5)\n",
      "memory usage: 125.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "X = vectorizer.transform(test['text_clean'])\n",
    "df_tfidf = pd.DataFrame(X.todense(), columns = vectorizer.get_feature_names())\n",
    "df_tfidf.drop(labels = ['id','location','text', 'target'], axis=1, inplace=True)\n",
    "test = test.join(df_tfidf)\n",
    "print(test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Columns: 5057 entries, keyword to target\n",
      "dtypes: bool(21), float64(5000), int64(31), object(5)\n",
      "memory usage: 292.7+ MB\n"
     ]
    }
   ],
   "source": [
    "target = train.target\n",
    "train.drop(columns=['target'], inplace=True)\n",
    "train['target'] = target\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/train_pre_processing_nlp_5000.csv', index=False)\n",
    "test.to_csv('data/test_pre_processing_nlp_5000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
