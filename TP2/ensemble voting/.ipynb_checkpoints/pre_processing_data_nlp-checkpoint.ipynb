{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import re\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 54 columns):\n",
      "keyword                              7552 non-null object\n",
      "location                             5080 non-null object\n",
      "text                                 7613 non-null object\n",
      "keyword_grouped                      7552 non-null object\n",
      "text_contain_keyword                 7613 non-null bool\n",
      "total_words                          7613 non-null int64\n",
      "total_upper_chars                    7613 non-null int64\n",
      "total_numbers_chars                  7613 non-null int64\n",
      "total_special_chars                  7613 non-null int64\n",
      "contain_question                     7613 non-null bool\n",
      "contain_link                         7613 non-null bool\n",
      "contain_hashtag                      7613 non-null bool\n",
      "contain_upper_words                  7613 non-null bool\n",
      "total_3_words                        7613 non-null int64\n",
      "total_4_words                        7613 non-null int64\n",
      "total_5_words                        7613 non-null int64\n",
      "total_6_words                        7613 non-null int64\n",
      "total_7_words                        7613 non-null int64\n",
      "total_8_words                        7613 non-null int64\n",
      "total_3_ormore_words                 7613 non-null int64\n",
      "total_4_ormore_words                 7613 non-null int64\n",
      "total_5_ormore_words                 7613 non-null int64\n",
      "total_6_ormore_words                 7613 non-null int64\n",
      "total_8_ormore_words                 7613 non-null int64\n",
      "total_3_orless_words                 7613 non-null int64\n",
      "total_4_orless_words                 7613 non-null int64\n",
      "total_5_orless_words                 7613 non-null int64\n",
      "total_6_orless_words                 7613 non-null int64\n",
      "total_8_orless_words                 7613 non-null int64\n",
      "subjectivity_text                    7613 non-null float64\n",
      "polarity_text                        7613 non-null float64\n",
      "stopword_count                       7613 non-null int64\n",
      "unique_word_count                    7613 non-null int64\n",
      "text_contain_word_location           7613 non-null bool\n",
      "len_location_cero_default            7613 non-null int64\n",
      "len_location_mean_default            7613 non-null int64\n",
      "total_words_location_cero_default    7613 non-null int64\n",
      "total_words_location_mean_default    7613 non-null int64\n",
      "text_contain_keyword_similarity      7613 non-null bool\n",
      "text_similarity_keyword              7613 non-null int64\n",
      "text_best_similarity_keyword         7613 non-null int64\n",
      "text_similarity_location             7613 non-null int64\n",
      "text_best_similarity_location        7613 non-null int64\n",
      "contain_words_100_true               7613 non-null bool\n",
      "contain_words_90_true                7613 non-null bool\n",
      "contain_words_90_false               7613 non-null bool\n",
      "contain_words_85_true                7613 non-null bool\n",
      "contain_words_85_false               7613 non-null bool\n",
      "contain_words_80_true                7613 non-null bool\n",
      "contain_words_80_false               7613 non-null bool\n",
      "contain_words_75_true                7613 non-null bool\n",
      "contain_words_70_true                7613 non-null bool\n",
      "contain_words_70_false               7613 non-null bool\n",
      "target                               7613 non-null int64\n",
      "dtypes: bool(17), float64(2), int64(31), object(4)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/train_pre_processing.csv')\n",
    "test = pd.read_csv('data/test_pre_processing.csv')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    text = re.compile(r'https?://\\S+|www\\.\\S+').sub(r' ', text)\n",
    "    text = re.compile(r'http\\S+').sub(r'', text)\n",
    "    text = re.compile(r'www\\S+').sub(r'', text)\n",
    "    text = re.compile(r'pic.twitter.com\\S+').sub(r' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [w for w in words if not w in STOPWORDS]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "def spellcheck(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_non_ascii(text):\n",
    "    return re.compile(r'[^A-Za-z0-9\\.\\'!\\?,\\$]').sub(r' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spaces(text):\n",
    "    # Reemplazar multiples espacios en uno\n",
    "    return re.sub('\\s{2,}', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = remove_url(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = clean_non_ascii(text)\n",
    "    text = clean_spaces(text)\n",
    "#    text = spellcheck(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text_clean'] = train.text.transform(lambda x: clean_text(str(x)))\n",
    "test['text_clean'] = test.text.transform(lambda x: clean_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5499    quarantined little corner bc sick office work piling need kinda day                     \n",
       "7436    CharlesDagnall Hes getting 50 I think Salt Wounds Rub In                                \n",
       "976     handbag fashion style Authentic Louis Vuitton Pochette Bosphore Shoulder Cross Body Bag \n",
       "4400    hot Funtenna hijacking computers send data sound waves Black Hat 2015 prebreak best     \n",
       "5953    camilacabello97 NOW IM INTERNALLY SCREAMING                                             \n",
       "Name: text_clean, dtype: object"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text_clean.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['mean_word_len'] = train.text.transform(lambda x: np.mean([len(word) for word in str(x).split()]))\n",
    "test['mean_word_len'] = test.text.transform(lambda x: np.mean([len(word) for word in str(x).split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text, n_gram=1):\n",
    "    token = []\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        filter_words = STOPWORDS.union(set(string.punctuation))\n",
    "        if w not in filter_words:\n",
    "            token.append(w)\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_bigrams = defaultdict(int)\n",
    "nondisaster_bigrams = defaultdict(int)\n",
    "\n",
    "for tweet in train.loc[train['target'] == 1]['text_clean']:\n",
    "    for word in get_ngrams(tweet, 2):\n",
    "        disaster_bigrams[word] += 1\n",
    "        \n",
    "for tweet in train.loc[train['target'] == 0]['text_clean']:\n",
    "    for word in get_ngrams(tweet, 2):\n",
    "        nondisaster_bigrams[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos mantenemos con los que tengan una frecuencia mayor o igual a 10 en desastre\n",
    "delete_keys = [k for k, v in disaster_bigrams.items() if v < 10]\n",
    "for key in delete_keys:\n",
    "    del disaster_bigrams[key]\n",
    "\n",
    "delete_keys = [k for k, v in nondisaster_bigrams.items() if v < 10]\n",
    "for key in delete_keys:\n",
    "    del nondisaster_bigrams[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature booleano para saber si el texto tiene un bigrama de desastre o no\n",
    "\n",
    "train['text_disaster_bigram'] = train.text_clean.transform(lambda x: any(disngram in disaster_bigrams.keys() for disngram in get_ngrams(x, 2)))\n",
    "train['text_nondisaster_bigram'] = train.text_clean.transform(lambda x: any(disngram in nondisaster_bigrams.keys() for disngram in get_ngrams(x, 2)))\n",
    "test['text_disaster_bigram'] = test.text_clean.transform(lambda x: any(disngram in disaster_bigrams.keys() for disngram in get_ngrams(x, 2)))\n",
    "test['text_nondisaster_bigram'] = test.text_clean.transform(lambda x: any(disngram in nondisaster_bigrams.keys() for disngram in get_ngrams(x, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6626\n",
       "True     987 \n",
       "Name: text_disaster_bigram, dtype: int64"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text_disaster_bigram'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6693\n",
       "True     920 \n",
       "Name: text_nondisaster_bigram, dtype: int64"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text_nondisaster_bigram'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_trigrams = defaultdict(int)\n",
    "nondisaster_trigrams = defaultdict(int)\n",
    "\n",
    "for tweet in train.loc[train['target'] == 1]['text_clean']:\n",
    "    for word in get_ngrams(tweet, 3):\n",
    "        disaster_trigrams[word] += 1\n",
    "        \n",
    "for tweet in train.loc[train['target'] == 0]['text_clean']:\n",
    "    for word in get_ngrams(tweet, 2):\n",
    "        nondisaster_trigrams[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos mantenemos con los que tengan una frecuencia mayor o igual a 3\n",
    "delete_keys = [k for k, v in disaster_trigrams.items() if v < 3]\n",
    "for key in delete_keys:\n",
    "    del disaster_trigrams[key]\n",
    "\n",
    "delete_keys = [k for k, v in nondisaster_trigrams.items() if v < 3]\n",
    "for key in delete_keys:\n",
    "    del nondisaster_trigrams[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature booleano para saber si el texto tiene un bigrama de desastre o no\n",
    "\n",
    "train['text_disaster_trigram'] = train.text_clean.transform(lambda x: any(disngram in disaster_trigrams.keys() for disngram in get_ngrams(x, 3)))\n",
    "train['text_nondisaster_trigram'] = train.text_clean.transform(lambda x: any(disngram in nondisaster_trigrams.keys() for disngram in get_ngrams(x, 2)))\n",
    "test['text_disaster_trigram'] = test.text_clean.transform(lambda x: any(disngram in disaster_trigrams.keys() for disngram in get_ngrams(x, 3)))\n",
    "test['text_nondisaster_trigram'] = test.text_clean.transform(lambda x: any(disngram in nondisaster_trigrams.keys() for disngram in get_ngrams(x, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6408\n",
       "True     1205\n",
       "Name: text_disaster_trigram, dtype: int64"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text_disaster_trigram'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    5306\n",
       "True     2307\n",
       "Name: text_nondisaster_trigram, dtype: int64"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text_nondisaster_trigram'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    norm='l2',\n",
    "    min_df=0,\n",
    "    smooth_idf=False,\n",
    "    max_features=15000)\n",
    "X = vectorizer.fit_transform(train['text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Columns: 15057 entries, keyword to zurich\n",
      "dtypes: bool(21), float64(15000), int64(31), object(5)\n",
      "memory usage: 873.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(X.todense(), columns = vectorizer.get_feature_names())\n",
    "df_tfidf.drop(labels = ['location', 'text', 'target'], axis=1, inplace=True)\n",
    "train = train.join(df_tfidf)\n",
    "print(train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Columns: 15057 entries, keyword to target\n",
      "dtypes: bool(21), float64(15000), int64(31), object(5)\n",
      "memory usage: 873.5+ MB\n"
     ]
    }
   ],
   "source": [
    "target = train.target\n",
    "train.drop(columns=['target'], inplace=True)\n",
    "train['target'] = target\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/train_pre_processing_dani.csv', index=False)\n",
    "test.to_csv('data/test_pre_processing_dani.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
